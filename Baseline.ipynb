{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://pre00.deviantart.net/4547/th/pre/f/2017/180/1/0/tao_pai_pai_by_wandertof-d9xtmm9.jpg height=\"500\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "This is the entrypoint for the competition, it:\n",
    "\n",
    "* Reads data from tweets' CSV files\n",
    "* Computes Bag of Words (BoW) from textual representations (tweets text)\n",
    "* Tests two models to find out which performs better\n",
    "* Predicts classes for the submission/benchmark tweets\n",
    "* Generates a suitable CSV for Kaggle InClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data representation\n",
    "\n",
    "The function `obtain_data_representation` performs the BoW transformation over the training set and applies it to both the train and test set.\n",
    "\n",
    "If no test set is provided, the input DataFrame is split into both train and test, 75% and 25% of the data respectively. This is done so as to be able to obtain an accuracy score, which will be the evaluation metric on Kaggle.\n",
    "\n",
    "BoW is computed through `CountVectorizer` class of `sklearn`, restricting it to at most 200 features. The process of finding the best words is done by the `fit` method, whereas transforming the text to numerical vectors (using the learnt features) is done by `transform`. Lastly, `fit_transform` does in a single step the learning and transforming process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def obtain_data_representation(df, test=None):\n",
    "    # If there is no test data, split the input\n",
    "    if test is None:\n",
    "        # Divide data in train and test\n",
    "        train, test = train_test_split(df, test_size=0.25)\n",
    "        df.airline_sentiment = pd.Categorical(df.airline_sentiment)\n",
    "    else:\n",
    "        # Otherwise, all is train\n",
    "        train = df\n",
    "        \n",
    "    # Create a Bag of Words (BoW), by using train data only\n",
    "    #cv = CountVectorizer(max_features=200)#, token_pattern=r'[A-Za-z]{3,}|no')\n",
    "    cv = CountVectorizer(max_features=300,stop_words='english', token_pattern=r'[A-Za-z@#]{3,}|no|yes|wtf|hrs|jfk', min_df = 5)\n",
    "    \n",
    "    #max_df=100)\n",
    "    \n",
    "    \n",
    "    #x_train = cv.fit_transform(train['text'])\n",
    "    #--\n",
    "    vectonizer = cv.fit(train['text'])\n",
    "    \n",
    "    print(cv.vocabulary_)\n",
    "    x_train = vectonizer.transform(train['text'])\n",
    "    #--\n",
    "    \n",
    "    y_train = train['airline_sentiment'].values\n",
    "    \n",
    "    #print(cv.get_feature_names())\n",
    "    print(\"vector shape: \", x_train.shape)\n",
    "    print(type(x_train))\n",
    "    print(x_train.toarray())\n",
    "    \n",
    "    # Obtain BoW for the test data, using the previously fitted one\n",
    "    x_test = cv.transform(test['text'])\n",
    "    try:\n",
    "        y_test = test['airline_sentiment'].values\n",
    "    except:\n",
    "        # It might be the submision file, where we don't have target values\n",
    "        y_test = None\n",
    "        \n",
    "    return {\n",
    "        'train': {\n",
    "            'x': x_train,\n",
    "            'y': y_train\n",
    "        },\n",
    "        'test': {\n",
    "            'x': x_test,\n",
    "            'y': y_test\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Thought this function might seem strange at first, the only thing to know is that training an `sklearn` model is always done the same way:\n",
    "\n",
    "```python\n",
    "# 1. Create the model\n",
    "model = BernoulliNB()\n",
    "\n",
    "# 2. Train with some data, where `x` are features and\n",
    "#    `y` is the target category\n",
    "model.fit(x, y)\n",
    "\n",
    "# 3. Predict new categories for test data (with which we\n",
    "#    have not trained!)\n",
    "y_pred = model.predict(test_x)\n",
    "```\n",
    "\n",
    "We might also obtain the accuracy score by using the function `accuracy_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_model(dataset, dmodel, *model_args, **model_kwargs):\n",
    "    # Create a Naive Bayes model\n",
    "    model = dmodel(*model_args, **model_kwargs)\n",
    "    \n",
    "    # Train it\n",
    "    model.fit(dataset['train']['x'], dataset['train']['y'])\n",
    "    \n",
    "    # Predict new values for test\n",
    "    y_pred = model.predict(dataset['test']['x'])\n",
    "    \n",
    "    # Print accuracy score unless its the submission dataset\n",
    "    if dataset['test']['y'] is not None:\n",
    "        score = accuracy_score(dataset['test']['y'], y_pred)\n",
    "        print(\"Model \", dmodel, \" score is: {}\".format(score))\n",
    "\n",
    "    # Done\n",
    "    return model, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'@southwestair': 5, 'yes': 298, 'great': 119, '@virginamerica': 8, 'like': 157, 'customer': 67, 'service': 232, '@jetblue': 4, '@united': 6, 'flight': 101, 'delayed': 74, 'hour': 134, 'amp': 20, 'won': 292, 'able': 9, 'tomorrow': 262, 'policy': 203, 'time': 258, 'airport': 17, 'want': 283, 'refund': 213, 'start': 240, '@usairways': 7, 'people': 195, 'working': 294, 'thank': 253, 'flt': 106, 'crew': 66, 'weather': 286, 'delay': 73, 'just': 148, 'good': 117, 'hold': 129, 'http': 137, '@americanair': 2, 'understand': 273, 'answer': 21, 'need': 179, 'home': 130, 'guys': 121, 'thanks': 254, 'help': 126, 'appreciate': 23, 'received': 212, 'bag': 31, 'night': 183, 'missing': 176, 'yesterday': 299, 'stuck': 244, 'disappointed': 83, 'better': 36, 'landed': 150, 'problem': 205, 'minutes': 173, 'cancelled': 48, 'reservation': 215, 'booked': 40, 'hours': 135, 'waited': 281, 'really': 208, 'food': 110, 'don': 87, 'tell': 251, 'actually': 10, 'way': 285, 'ticket': 256, 'check': 54, 'online': 187, 'fleet': 100, 'fleek': 99, 'flightled': 103, 'seats': 227, 'credit': 65, 'team': 250, 'wait': 280, 'money': 177, 'paid': 190, 'early': 89, 'different': 82, 'agents': 12, 'rude': 219, 'employees': 91, 'airlines': 16, 'does': 84, 'let': 156, 'website': 287, 'hrs': 136, 'going': 116, 'baggage': 32, 'airline': 15, 'joke': 147, 'times': 259, 'bad': 30, 'luggage': 164, 'come': 60, 'problems': 206, 'dca': 72, 'sitting': 235, 'gate': 113, 'issues': 143, 'sure': 246, 'did': 80, 'know': 149, 'booking': 41, 'flights': 105, 'united': 274, 'app': 22, 'say': 223, 'tickets': 257, 'vegas': 278, 'change': 52, 'waiting': 282, 'pay': 194, 'person': 196, 'flying': 108, 'air': 14, 'think': 255, 'told': 261, 'says': 225, 'late': 151, 'flightr': 104, 'terrible': 252, 'min': 171, 'bags': 33, 'staff': 239, 'plane': 200, 'send': 229, 'stranded': 243, 'passengers': 192, 'real': 207, 'said': 221, 'open': 188, 'left': 155, 'jfk': 145, 'sfo': 233, 'seat': 226, 'response': 216, 'fix': 98, 'issue': 142, 'past': 193, 'unacceptable': 272, 'fly': 107, 'day': 70, 'maybe': 167, 'extra': 94, 'week': 288, 'family': 95, 'charge': 53, 'tried': 267, 'available': 27, 'southwest': 237, 'amazing': 19, 'right': 218, 'checked': 55, 'rebooked': 211, 'looking': 161, 'second': 228, 'missed': 175, 'nice': 182, 'lost': 162, 'miles': 170, 'got': 118, 'line': 158, 'hotel': 133, 'helpful': 127, 'frustrating': 112, 'today': 260, 'email': 90, 'new': 181, 'needs': 180, 'poor': 204, 'denver': 76, 'board': 37, 'mechanical': 168, 'tonight': 263, 'asked': 24, 'hope': 131, 'trying': 269, 'update': 275, 'tarmac': 249, 'year': 297, 'having': 125, 'dallas': 69, 'morning': 178, 'making': 166, 'jetblue': 144, 'sent': 230, 'wrong': 296, 'use': 276, 'voucher': 279, 'trip': 268, 'talk': 248, 'phone': 198, 'doesn': 85, 'make': 165, 'rep': 214, 'days': 71, 'weeks': 289, 'took': 264, 'car': 49, 'travel': 265, 'mins': 172, 'agent': 11, 'calling': 46, 'helping': 128, 'vacation': 277, 'awesome': 28, 'connection': 63, 'supposed': 245, 'dfw': 79, 'work': 293, 'message': 169, 'rebook': 210, 'best': 35, 'experience': 93, 'ground': 120, 'instead': 140, 'customers': 68, 'leaving': 154, 'haven': 124, 'job': 146, 'card': 50, '#unitedairlines': 1, 'twitter': 271, 'free': 111, 'wifi': 291, 'didn': 81, 'boarding': 38, 'plus': 202, 'called': 45, 'number': 184, 'horrible': 132, 'book': 39, 'pilot': 199, 'getting': 115, 'leave': 153, 'awful': 29, '@delta': 3, 'care': 51, 'departure': 77, 'ago': 13, 'info': 139, 'worst': 295, 'claim': 57, 'bos': 42, 'happy': 123, 'site': 234, '#fail': 0, 'isn': 141, 'calls': 47, 'attendants': 26, 'class': 58, 'delays': 75, 'follow': 109, 'doing': 86, 'status': 241, 'traveling': 266, 'long': 159, 'business': 44, 'love': 163, 'stop': 242, 'lax': 152, 'saying': 224, 'reason': 209, 'sorry': 236, 'couldn': 64, 'ewr': 92, 'gave': 114, 'taking': 247, 'attendant': 25, 'pass': 191, 'ord': 189, 'old': 186, 'flighted': 102, 'phl': 197, 'runway': 220, 'seriously': 231, 'connecting': 62, 'san': 222, 'earlier': 88, 'miss': 174, 'hung': 138, 'went': 290, 'planes': 201, 'airways': 18, 'offer': 185, 'broken': 43, 'far': 96, 'look': 160, 'company': 61, 'finally': 97, 'ridiculous': 217, 'twice': 270, 'wasn': 284, 'speak': 238, 'clt': 59, 'believe': 34, 'half': 122, 'chicago': 56, 'desk': 78}\n",
      "vector shape:  (4680, 300)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "Model  <class 'sklearn.naive_bayes.BernoulliNB'>  score is: 0.8198717948717948\n",
      "Model  <class 'sklearn.neighbors.classification.KNeighborsClassifier'>  score is: 0.6480769230769231\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "df = pd.read_csv('tweets_public.csv', index_col='tweet_id')\n",
    "df = df.loc[df.airline_sentiment_confidence > .99]\n",
    "dataset = obtain_data_representation(df)\n",
    "\n",
    "# Train a Bernoulli Naive Bayes\n",
    "modelNB, _ = train_model(dataset, BernoulliNB)\n",
    "\n",
    "# Train a K Nearest Neighbors Classifier\n",
    "modelKN, _ = train_model(dataset, KNeighborsClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit file\n",
    "\n",
    "Once we have found the best model (BernoulliNB for the above simple test), we can train it with all the data (that is, avoid doing a train/test split) and predict sentiments for the real submission data.\n",
    "\n",
    "This cell below performs exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission_11_14_2017-19_47_49.csv\n",
      "Upload it to Kaggle InClass\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "def create_submit_file(df_submission, ypred):\n",
    "    date = datetime.datetime.now().strftime(\"%m_%d_%Y-%H_%M_%S\")\n",
    "    filename = 'submission_' + date + '.csv'\n",
    "    \n",
    "    df_submission['airline_sentiment'] = ypred\n",
    "    df_submission[['airline_sentiment']].to_csv(filename)\n",
    "    \n",
    "    print('Submission file created: {}'.format(filename))\n",
    "    print('Upload it to Kaggle InClass')\n",
    "\n",
    "    \n",
    "# Read submission and retrain with whole data\n",
    "df_submission = pd.read_csv('tweets_submission.csv', index_col='tweet_id')\n",
    "# We use df_submision as test, otherwise it would split df in train/test\n",
    "submission_dataset = obtain_data_representation(df, df_submission)\n",
    "# Predict for df_submission\n",
    "_, y_pred = train_model(submission_dataset, BernoulliNB)\n",
    "\n",
    "# Create submission file with obtained y_pred\n",
    "create_submit_file(df_submission, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
